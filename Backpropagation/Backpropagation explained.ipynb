{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: let say we define a input $x = [1, 2]$ and output is $y = [1]$. We are going to use 3 layered network with 2 neurons in first layer, 2 neurons in second layer and 1 neuron in last layer. As a loss function we are going to use $MSE$\n",
    "\n",
    "\n",
    "**TODO:** image with inputs and all notation\n",
    "\n",
    "\n",
    "\n",
    "**Notation:**\n",
    "- **L** - layer where *L* is output layer, _L-1_ is previous to last and so on\n",
    "- **w** - weight on certain conection\n",
    "- **b** - bias on certain neuron\n",
    "- **z** - sum of weights and values. It's not output from neuron because an activation function is not included\n",
    "- **a** - input value or output value from some neuron (that is input value to next layer). It's a __z__ with activation function\n",
    "\n",
    "\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**In order to see how much we are from our result we need some kind of a measure, for this example we are going to use a loss function called *Mean squared error* expressed as follows:**\n",
    "\n",
    "$MSE = \\sum_{i=1}^n{(y - \\hat{y})^2}$\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "**We see we are far of the right solution so we need to optimize it**\n",
    "\n",
    "For that we will use gradient descent. The thing that we tried to optimize is our **loss function** which we want to be minimal. Our cost function is $MSE$ and if we remember is expressed as (we will use $C$ notation for loss/cost):\n",
    "\n",
    "\n",
    "$C = (\\hat{y} - y)^2$\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now from this pile of parameters we can tune the weights $w$ and biases $b$. We mentioned we are going to use gradient descent. For that we need to do a derivitive of $C$, but we cannot do it as function involves multiple variables.\n",
    "What we can do is do a partial derivitive of $C$ with respect to a certain weight $w$ or bias $b$.\n",
    "\n",
    "<!---\n",
    "Optimize weight $w_8$:\n",
    "\n",
    "$ \\frac{\\partial C}{\\partial w_8} = 2 * (z_{n3} * w_7 + z_{n4} * w_8 + b_5) * (z_{n3} * w_7 + z_{n4} + b_5) $\n",
    "\n",
    "$ \\frac{\\partial C}{\\partial w_8} = 2 * (86 * 7 + 106 + 5) * (86 * 7 + 106 + 5) = 1,016,738$\n",
    "\n",
    "\n",
    "Our new weight $w_8 = 8 - 1016738 = -1,016,730$\n",
    "\n",
    "\n",
    "**Sidenote:** If we run the calulation now we will get output value of $\\hat{y} = -107773621$ and $MSE = $\n",
    "-->\n",
    "\n",
    "\n",
    "$\\frac{\\partial C_o}{\\partial w^{(L)}} = \\frac{\\partial z^{(L)}}{\\partial w^{(L)}} \\frac{\\partial a^{(L)}} {\\partial z^{(L)}} \\frac{\\partial C_o}{\\partial a^{(L)}}$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\n",
    "\\frac{\\partial C_o}{\\partial w_8} = \\frac{\\partial z^{(L)}}{\\partial w^{(L)}} \\frac{\\partial a^{(L)}} {\\partial z^{(L)}} \\frac{\\partial C_o}{\\partial a^{(L)}}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cost or loss can be calculated as follows:**\n",
    "$C_o = (a^{(L)} - y)^2$\n",
    "\n",
    "<br>\n",
    "\n",
    "**Then we derivative a the cost with respect to a weight:** \n",
    "$\\frac{\\partial C_o}{\\partial w^{(L)}} = \\frac{\\partial z^{(L)}}{\\partial w^{(L)}} \\frac{\\partial a^{(L)}} {\\partial z^{(L)}} \\frac{\\partial C_o}{\\partial a^{(L)}}$\n",
    "\n",
    "\n",
    "<br><hr>\n",
    "$C_o$ with respect to $a^{(L)}$ derivatived will be:\n",
    "\n",
    "$\\frac{\\partial C_o}{\\partial a^{(L)}} = 2(a^{(L)}-y)$\n",
    "\n",
    "\n",
    "<hr>\n",
    "This next derivative is going to be just a derivate of activation function:\n",
    "\n",
    "$\\frac{\\partial a_{(L)}}{\\partial z^{(L)}} = \\sigma'(z^{(L)}) $ \n",
    "\n",
    "<hr>\n",
    "This derivative will be an activated output from previous layer:\n",
    "\n",
    "$\\frac{\\partial z^{(L)}}{\\partial w^{(L)}} = a ^{(L-1)}$\n",
    "\n",
    "\n",
    "<hr>\n",
    "Derivative for bias will be:\n",
    "\n",
    "$\\frac{\\partial C_o}{\\partial b^{(L)}} = \\frac{\\partial z^{(L)}}{\\partial b^{(L)}} \\frac{\\partial a^{(L)}} {\\partial z^{(L)}} \\frac{\\partial C_o}{\\partial a^{(L)}}$\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "When we have a multiple output neurons, our cost function will be:\n",
    "\n",
    "$C_o = \\sum_{j=0}^{n_{L-1}}{(a_j^{(L)} - y_j)^2}$\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "When calculating a derivative for neuron that will affect multiple neurons in the next layer ($L$), we must calculate the cost functions for all those neurons that he is connected to:\n",
    "\n",
    "$\\frac{\\partial C_o}{\\partial a^{(L-1)}} = \\sum_{i=0}^{n_{L-1}}{\\frac{\\partial z^{(L)}}{\\partial a^{(L-1)}}\n",
    "\\frac{\\partial a^{(L)}}{\\partial z^{(L)}}\n",
    "\\frac{\\partial C_0}{\\partial a^{(L)}}}$\n",
    "\n",
    "<hr>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super-simple example\n",
    "\n",
    "<pre>\n",
    "   w\n",
    "x ----- [n1] ----> $\\hat{y}$\n",
    "</pre>\n",
    "\n",
    "Let say $x = 1, w = 2, y=1$\n",
    "\n",
    "<hr>\n",
    "\n",
    "If we forward propagate our input value we get:\n",
    "\n",
    "$\\hat{y} = x * w = 1 * 2 = 2$\n",
    "\n",
    "<hr>\n",
    "\n",
    "We now need to measure how far off we are from our desired output and for that we are going to use $MSE$ expressed as:\n",
    "\n",
    "$MSE = \\sum_{i=1}^n{(y - \\hat{y})^2}$\n",
    "\n",
    "Then our loss will be:\n",
    "\n",
    "$MSE = (1 - 2)^2 = 1$\n",
    "\n",
    "<hr>\n",
    "\n",
    "In order to optimize our network we must optimize our loss/cost. Our loss/cost is depended on output of last neuron, meaning we somehow must optimize it's output to help loss function to optimize. \n",
    "We do that by taking a partial derivitive of loss (notation is $C$) function with respect to last neuron output value:\n",
    "\n",
    "$\\frac{\\partial C}{\\partial a} = -2 (y - a)$\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "But $a$ is just a output from neuron which is determined by the weight $w$. So we need to change $w$ to change $a$ who will optimize our loss function. To do so we need to do a partial derivitive of $a$ with respect to $w$:\n",
    "\n",
    "(Sidenote: $a$ was just $w * x$)\n",
    "\n",
    "$\\frac{\\partial a}{\\partial w} = x$\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "Now when we computed partial derivitives we can calculate partial derivitive of cost with respect to $w$:\n",
    "\n",
    "$\\frac{\\partial C}{\\partial w} = \\frac{\\partial a}{\\partial w} \\frac{\\partial C}{\\partial a}$\n",
    "\n",
    "and now we just insert what we got before:\n",
    "\n",
    "$ = x * (-2(y - a))$\n",
    "\n",
    "<hr>\n",
    "\n",
    "Now we can update our weight. We are going to include one more thing called learning rate $lr$ and set it to $0.1$ for now. Then we only need to calculate our new weight:\n",
    "\n",
    "$w = w - lr * (x * (-2(y - a))) = 1.8$\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "If we run the example $x$ again we see that $\\hat{y} = 1.8$ and $MSE = 0.64$ which is better than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8\n"
     ]
    }
   ],
   "source": [
    "x = 1\n",
    "w = 2\n",
    "y = 1\n",
    "lr = 0.1\n",
    "\n",
    "a = x * w\n",
    "\n",
    "w = w - lr * (x * (-2 * (y - a)))\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [array([[2.]], dtype=float32)]\n",
      "y_hat: [[2.]]\n",
      "weights_1: [array([[1.8]], dtype=float32)]\n",
      "y_hat_1: [[1.8]]\n"
     ]
    }
   ],
   "source": [
    "# Validation with tensorflow\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.optimizers import SGD\n",
    "\n",
    "x = [1]\n",
    "y = 1\n",
    "w = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=1, use_bias=False))\n",
    "model.compile(loss='mse', optimizer=SGD(lr=0.1), metrics=['mse'])\n",
    "\n",
    "model.set_weights([[[w]]])\n",
    "print(\"weights:\", model.get_weights())\n",
    "print(\"y_hat:\", model.predict(np.array(x).reshape(1, 1)))\n",
    "\n",
    "model.fit(np.array([x]), [y], epochs=1, verbose=0)\n",
    "print(\"weights_1:\", model.get_weights())\n",
    "print(\"y_hat_1:\", model.predict(np.array(x).reshape(1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super-simple example 2\n",
    "\n",
    "<pre>\n",
    "   w1         w2\n",
    "x ----- [n1] ----- [n2] -----> $\\hat{y}$\n",
    "</pre>\n",
    "\n",
    "Let say $x = 1, w_1 = 2, w_2 = 3, y=1$\n",
    "\n",
    "Additional notation:\n",
    "\n",
    "$a_1$: output from neuron $n_1$\n",
    "\n",
    "$a_2$: output from neuron $n_2$\n",
    "\n",
    "<hr>\n",
    "\n",
    "After forward propagation we get:\n",
    "\n",
    "$a_1 = x * w_1 = 1 * 2 = 2$\n",
    "\n",
    "$a_2 = a_1 * w_2 = 2 * 3 = 6$\n",
    "\n",
    "or\n",
    "\n",
    "$\\hat{y} = w_2 (x * w_1) = 3 (1 * 2) = 6$\n",
    "\n",
    "<hr>\n",
    "\n",
    "Our $MSE$ loss will be:\n",
    "\n",
    "$MSE = (1 - 6)^2 = 25$\n",
    "\n",
    "<hr>\n",
    "\n",
    "Now we optimize our loss/cost:\n",
    "\n",
    "$\\frac{\\partial C}{\\partial a_2} = -2 (y - a_2)$\n",
    "\n",
    "<hr>\n",
    "\n",
    "Next we need to find a derivitive for $a_2$ with respect to $w_2$:\n",
    "\n",
    "(Sidenote: $a_2$ was $w_2 * a_1$)\n",
    "\n",
    "$\\frac{\\partial a_2}{\\partial w_2} = a_1$\n",
    "\n",
    "<hr>\n",
    "\n",
    "We are now ready to calculate partial derivitive of cost with respect to $w_2$:\n",
    "\n",
    "$\\frac{\\partial C}{\\partial w_2} = a_1 * (-2 (y - a_2))$\n",
    "\n",
    "<hr>\n",
    "\n",
    "Now update $w_2$:\n",
    "\n",
    "$w_2 = w_2 - lr * (a_1 * (-2 (y - a_2))) = 1$\n",
    "\n",
    "<hr>\n",
    "If we run the example $x$ again we see that $\\hat{y} = 2$ and $MSE = 1$ which is way better than before.\n",
    "\n",
    "But we are not done yet! We need to update $w_1$ too.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Meaning we need to find partial derivitive of $C$ with respect to $w_1$:\n",
    "\n",
    "$\\frac{\\partial C}{\\partial w_1} = \\frac{\\partial a_1}{\\partial w_1} \\frac{\\partial a_2}{\\partial a_1} \\frac{\\partial C}{\\partial a_2}$\n",
    "\n",
    "<hr>\n",
    "\n",
    "We calculated the $\\frac{\\partial C}{\\partial a_2}$ before and we can reuse it here, which means we only need to calculate $\\frac{\\partial a_1}{\\partial w_1}$ and $\\frac{\\partial a_2}{\\partial a_1}$:\n",
    "\n",
    "(Sidenote: $\\frac{\\partial C}{\\partial a_2} = -2 (y - a_2)$)\n",
    "\n",
    "<hr>\n",
    "\n",
    "(Sidenote: $a_2 = a_1 * w_2$)\n",
    "\n",
    "$\\frac{\\partial a_2}{\\partial a_1} = w_2$\n",
    "\n",
    "<hr>\n",
    "\n",
    "(Sidenote: $a_1 = x * w_1$)\n",
    "\n",
    "$\\frac{\\partial a_1}{\\partial w_1} = x$\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "Now our complete derivitive for derivitive $C$ with respect to $w_1$ would be:\n",
    "\n",
    "$\\frac{\\partial C}{\\partial w_1} = x * w_2 * (-2 (y - a_2))$\n",
    "\n",
    "<hr>\n",
    "\n",
    "Now update $w_1$:\n",
    "\n",
    "$w_1 = w_1 - lr * (x * w_2 * (-2 (y - a_2))) = -1$\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "Running the example gives us $\\hat{y} = -1$ and $MSE = 4$ which is better than we started with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New w_2: 1.0\n",
      "New w_1: -1.0\n",
      "Update network...\n",
      "-1.0 -1.0\n",
      "y_hat:  -1.0 MSE:  4.0\n"
     ]
    }
   ],
   "source": [
    "x = 1\n",
    "w_1 = 2\n",
    "w_2 = 3\n",
    "y=1\n",
    "lr = 0.1\n",
    "\n",
    "a_1 = x * w_1\n",
    "a_2 = a_1 * w_2\n",
    "\n",
    "new_w_2 = w_2 - lr * (a_1 * (-2 * (y - a_2)))\n",
    "print(\"New w_2:\", new_w_2)\n",
    "\n",
    "new_w_1 = w_1 - lr * (x * w_2 * (-2 * (y - a_2)))\n",
    "print(\"New w_1:\", new_w_1)\n",
    "\n",
    "print(\"Update network...\")\n",
    "\n",
    "w_2 = new_w_2\n",
    "w_1 = new_w_1\n",
    "\n",
    "a_1 = x * w_1\n",
    "a_2 = a_1 * w_2\n",
    "print(a_1,a_2)\n",
    "print(\"y_hat: \", a_2, \"MSE: \", (y-a_2)**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**May as well do another epoch**\n",
    "\n",
    "Lets recap our values:\n",
    "\n",
    "$x = 1, w_1 = -1, w_2 = 1, y = 1$\n",
    "\n",
    "$a_1 = x * w_1 = -1$\n",
    "\n",
    "$a_2 = a_1 * w_2 = 0.2 * 1 = -1$\n",
    "\n",
    "\n",
    "$\\hat{y} = -1$, $MSE = 4$\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Update $w_2$:**\n",
    "\n",
    "$\\frac{\\partial C}{\\partial a_2} = - 2(y - a_2)$\n",
    "\n",
    "<hr>\n",
    "\n",
    "(Sidenote: $a_2 = w_2 * a_1$)\n",
    "\n",
    "$\\frac{\\partial a_2}{\\partial w_2} = a_1$\n",
    "\n",
    "<hr>\n",
    "\n",
    "$\\frac{\\partial C}{\\partial w_2} = a_1 * (-2(y - a_2))$\n",
    "\n",
    "<hr>\n",
    "\n",
    "$w_2 = w_2 - lr * (a_1 * (-2(y - a_2))) = 0.6$\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Update $w_1$:**\n",
    "\n",
    "$\\frac{\\partial C}{\\partial w_1} = \\frac{\\partial a_1}{\\partial w_1} \\frac{\\partial a_2}{\\partial a_1} \\frac{\\partial C}{\\partial a_2}$\n",
    "\n",
    "\n",
    "(Sidenote: $\\frac{\\partial C}{\\partial a_2} = -2(y - a_2)$)\n",
    "\n",
    "<hr>\n",
    "\n",
    "(Sidenote: $a_2 = a_1 * w_2$)\n",
    "\n",
    "$\\frac{\\partial a_2}{\\partial a_1} = w_2$\n",
    "\n",
    "<hr>\n",
    "\n",
    "(Sidenote: $a_1 = x * w_1$)\n",
    "\n",
    "$\\frac{\\partial a_1}{\\partial w_1} = x$\n",
    "\n",
    "<hr>\n",
    "\n",
    "$\\frac{\\partial C}{\\partial w_1} = x * w_2 * (- 2(y - a_2))$\n",
    "\n",
    "<hr>\n",
    "\n",
    "$w_1 = w_1 - lr * (x * w_2 * (- 2(y - a_2))) = -0.6$\n",
    "\n",
    "<hr>\n",
    "\n",
    "If we check our example now we well get $\\hat{y} = -0.36$ and $MSE = 1.85$ which is slow but good change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New w_2: 0.6\n",
      "New w_1: -0.6\n",
      "Update network...\n",
      "-0.6 -0.36\n",
      "y_hat:  -0.36 MSE:  1.8495999999999997\n"
     ]
    }
   ],
   "source": [
    "x = 1\n",
    "w_1 = -1\n",
    "w_2 = 1\n",
    "y = 1\n",
    "lr = 0.1\n",
    "\n",
    "a_1 = x * w_1\n",
    "a_2 = a_1 * w_2\n",
    "\n",
    "new_w_2 = w_2 - lr * (a_1 * (-2 * (y - a_2)))\n",
    "print(\"New w_2:\", new_w_2)\n",
    "\n",
    "new_w_1 = w_1 - lr * (x * w_2 * (-2 * (y - a_2)))\n",
    "print(\"New w_1:\", new_w_1)\n",
    "\n",
    "print(\"Update network...\")\n",
    "\n",
    "w_2 = new_w_2\n",
    "w_1 = new_w_1\n",
    "\n",
    "a_1 = x * w_1\n",
    "a_2 = a_1 * w_2\n",
    "print(a_1,a_2)\n",
    "print(\"y_hat: \", a_2, \"MSE: \", (y-a_2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat: [[6.]]\n",
      "weights: [array([[2.]], dtype=float32), array([[3.]], dtype=float32)]\n",
      "y_hat_1: [[-1.]]\n",
      "weights_1: [array([[-1.]], dtype=float32), array([[1.]], dtype=float32)]\n",
      "y_hat_2: [[-0.36]]\n",
      "weights_2: [array([[-0.6]], dtype=float32), array([[0.6]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Validation with tensorflow\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.optimizers import SGD\n",
    "\n",
    "x = [1]\n",
    "y = 1\n",
    "w_1 = 2\n",
    "w_2 = 3\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=1, use_bias=False))\n",
    "model.add(Dense(1, use_bias=False))\n",
    "model.compile(loss='mse', optimizer=SGD(lr=0.1), metrics=['mse'])\n",
    "\n",
    "model.set_weights([[[w_1]], [[w_2]]])\n",
    "print(\"weights:\", model.get_weights())\n",
    "print(\"y_hat:\", model.predict(np.array(x).reshape(1, 1)))\n",
    "\n",
    "model.fit(np.array([x]), [y], epochs=1, verbose=0)\n",
    "print(\"weights_1:\", model.get_weights())\n",
    "print(\"y_hat_1:\", model.predict(np.array(x).reshape(1, 1)))\n",
    "\n",
    "model.fit(np.array([x]), [y], epochs=1, verbose=0)\n",
    "print(\"weights_2:\", model.get_weights())\n",
    "print(\"y_hat_2:\", model.predict(np.array(x).reshape(1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Lets mix it up a bit!\n",
    "**We must include bias and activation function in our network**\n",
    "\n",
    "**Lets consider following example:**\n",
    "\n",
    "\n",
    "<pre>\n",
    "   b1___        b2__\n",
    "   w1   \\       w2  \\\n",
    "x ----- [n1]\\ ----- [n2]\\ -----> $\\hat{y}$\n",
    "</pre>\n",
    "\n",
    "\\ - means activation function\n",
    "\n",
    "Let say $x = 1, w_1 = 1, b_1 = 1, b_2 = 2, w_2 = 2, y=2$\n",
    "\n",
    "**Notation:**\n",
    "\n",
    "$x$: input value\n",
    "\n",
    "$w_1, w_2$: weights\n",
    "\n",
    "$b_1, b_2$: biases for neurons\n",
    "\n",
    "$z_1, z_2$: sum of input value and weights\n",
    "\n",
    "$a_1, a_2$: output from neuron ($z_1, z_2$ with activation function)\n",
    "\n",
    "<br>\n",
    "\n",
    "**Formulas:**\n",
    "\n",
    "For activation function we will use $\\sigma$:\n",
    "\n",
    "$\\displaystyle \\sigma(x)={\\frac {1}{1+e^{-x}}}$\n",
    "\n",
    "Derivitive will be:\n",
    "\n",
    "$\\displaystyle \\sigma'(x)= \\sigma(x) * (1 - \\sigma(x))$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "$z_1 = x * w_1 + b_1$\n",
    "\n",
    "$a_1 = \\sigma(z_1)$\n",
    "\n",
    "$z_2 = a_1 * w_2 + b_2$\n",
    "\n",
    "$a_2 = \\sigma(z_2)$\n",
    "\n",
    "<hr>\n",
    "\n",
    "When we run $x$ through network we get $\\hat{y} = 0.9772$ and $MSE = 1.046$\n",
    "\n",
    "<hr>\n",
    "\n",
    "**First things first! We now have some extra parameters which we need to take into account.**\n",
    "\n",
    "Our derivitive of $C$ with respect to $w$ will now depend on activation function as well:\n",
    "\n",
    "$\\displaystyle \\frac{\\partial C}{\\partial w} = \\frac{\\partial z}{\\partial w} \\frac{\\partial a}{\\partial z} \\frac{\\partial C}{\\partial a}$\n",
    "\n",
    "\n",
    "But we need to update our bias too and it will look very similar to formula for $w$:\n",
    "\n",
    "$\\displaystyle \\frac{\\partial C}{\\partial b} = \\frac{\\partial z}{\\partial b} \\frac{\\partial a}{\\partial z} \\frac{\\partial C}{\\partial a}$\n",
    "\n",
    "\n",
    "Okay, we are ready.\n",
    "\n",
    "<hr>\n",
    "\n",
    "As always we start by finding a derivitive of our loss/cost function:\n",
    "\n",
    "$\\frac{\\partial C}{\\partial a_2} = -2 (y - a_2)$\n",
    "\n",
    "<hr>\n",
    "\n",
    "(Sidenote: $a_2 = \\sigma(z_2)$)\n",
    "\n",
    "$\\frac{\\partial a_2}{\\partial z_2} = \\sigma(z_2) * (1 - \\sigma(z_2))$\n",
    "\n",
    "<hr>\n",
    "\n",
    "(Sidenote: $z_2 = a_1 * w_2 + b_2$)\n",
    "\n",
    "$\\frac{\\partial z_2}{\\partial w_2} = a_1$\n",
    "\n",
    "<hr>\n",
    "\n",
    "$\\frac{\\partial C}{\\partial w_2} = (a_1) * (\\sigma(z_2) * (1 - \\sigma(z_2))) * (-2 (y - a_2))$\n",
    "\n",
    "<hr>\n",
    "\n",
    "$w_2 = w_2 - lr * ((a_1) * (\\sigma(z_2) * (1 - \\sigma(z_2))) * (-2 (y - a_2)))$\n",
    "\n",
    "$w_2 = 2.004$\n",
    "\n",
    "<hr>\n",
    "\n",
    "On to update $b_2$:\n",
    "\n",
    "(Sidenote: $z_2 = a_1 * w_2 + b_2$)\n",
    "\n",
    "$\\frac{\\partial z_2}{\\partial b_2} = 1$\n",
    "\n",
    "\n",
    "$\\frac{\\partial C}{\\partial b_2} = (1) * (\\sigma(z_2) * (1 - \\sigma(z_2))) * (-2 (y - a_2))$\n",
    "\n",
    "<hr>\n",
    "\n",
    "$b_2 = b_2 - lr * ((1) * (sigma(z_2) * (1 - sigma(z_2))) * (-2 * (y - a_2)))$\n",
    "\n",
    "$b_2 = 2.0045$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**On to update $w_1$ and $b_1$:**\n",
    "\n",
    "$\\displaystyle \\frac{\\partial C}{\\partial w_1} = \\frac{\\partial z_1}{\\partial w_1} \\frac{\\partial a_1}{\\partial z_1} \\frac{\\partial z_2}{\\partial a_1} \\frac{\\partial a_2}{\\partial z_2} \\frac{\\partial C}{\\partial a_2}$\n",
    "\n",
    "$\\displaystyle \\frac{\\partial C}{\\partial b_1} = \\frac{\\partial z_1}{\\partial b_1} \\frac{\\partial a_1}{\\partial z_1} \\frac{\\partial z_2}{\\partial a_1} \\frac{\\partial a_2}{\\partial z_2} \\frac{\\partial C}{\\partial a_2}$\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Calculate gradient for $w_1$:**\n",
    "\n",
    "(Sidenote: $z_2 = a_1 * w_2 + b_2$)\n",
    "\n",
    "$\\frac{\\partial z_2}{\\partial a_1} = w_2 + 0$\n",
    "\n",
    "<hr>\n",
    "\n",
    "(Sidenote: $a_1 = \\sigma(z_1)$)\n",
    "\n",
    "$\\frac{\\partial a_1}{\\partial z_1} = \\sigma(z_1) * (1 - \\sigma(z_1))$\n",
    "\n",
    "<hr>\n",
    "\n",
    "(Sidenote: $z_1 = x * w_1 + b_1$)\n",
    "\n",
    "$\\frac{\\partial z_1}{\\partial w_1} = x$\n",
    "\n",
    "<hr>\n",
    "\n",
    "Finally we get:\n",
    "\n",
    "$\\frac{\\partial C}{\\partial w_1} = \n",
    "(x) \n",
    "* \n",
    "(\\sigma(z_1) * (1 - \\sigma(z_1)))\n",
    "*\n",
    "(w_2)\n",
    "*\n",
    "(\\sigma(z_2) * (1 - \\sigma(z_2)))\n",
    "*\n",
    "(-2 (y -a_2))$\n",
    "\n",
    "\n",
    "$w_1 = w_1 - lr * ((x) * (\\sigma(z_1) * (1 - \\sigma(z_1))) * (w_2) * (\\sigma(z_2) * (1 - \\sigma(z_2))) * (-2 (y -a_2)))$\n",
    "\n",
    "$w_1 = 1.00095$\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Calculate gradient for $b_1$:**\n",
    "\n",
    "(Sidenote: $z_1 = x * w_1 + b_1$)\n",
    "\n",
    "$\\frac{\\partial z_1}{\\partial b_1} = 1$\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "Then we get:\n",
    "\n",
    "$\\displaystyle \\frac{\\partial C}{\\partial b_1} = \n",
    "(1)\n",
    "*\n",
    "(\\sigma(z_1) * (1 - \\sigma(z_1)))\n",
    "*\n",
    "(w_2)\n",
    "*\n",
    "(\\sigma(z_2) * (1 - \\sigma(z_2)))\n",
    "*\n",
    "(-2 (y - a_2))$\n",
    "\n",
    "\n",
    "$b_1 = b_1 - lr * ((1) * (\\sigma(z_1) * (1 - \\sigma(z_1))) * (w_2) * (\\sigma(z_2) * (1 - \\sigma(z_2))) * (-2 (y - a_2)))$\n",
    "\n",
    "$b_1 = 1.00095$\n",
    "\n",
    "<hr>\n",
    "\n",
    "**There we have it! One epoch is done.**\n",
    "\n",
    "Our initial result was: $\\hat{y}=0.9772$ and $MSE=1.046$\n",
    "\n",
    "And after one epoch result is: $\\hat{y}=0.9774$ and $MSE=1.045$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New w_2: 2.0040000160377818\n",
      "New b_2: 2.0045413593412063\n",
      "New w_1: 1.000953627199678\n",
      "New b_1: 1.000953627199678\n",
      "Update network...\n",
      "y_hat:  0.9774686759735803 MSE:  1.045570308615223\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def sigma(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "x = 1\n",
    "y = 2\n",
    "w_1 = 1\n",
    "b_1 = 1\n",
    "w_2 = 2\n",
    "b_2 = 2\n",
    "\n",
    "z_1 = x * w_1 + b_1\n",
    "a_1 = sigma(z_1)\n",
    "z_2 = a_1 * w_2 + b_2\n",
    "a_2 = sigma(z_2)\n",
    "\n",
    "\n",
    "new_w_2 = w_2 - lr * ((a_1) * (sigma(z_2) * (1 - sigma(z_2))) * (-2 *(y - a_2)))\n",
    "print(\"New w_2:\", new_w_2)\n",
    "\n",
    "new_b_2 = b_2 = b_2 - lr * ((1) * (sigma(z_2) * (1 - sigma(z_2))) * (-2 * (y - a_2)))\n",
    "print(\"New b_2:\", new_b_2)\n",
    "\n",
    "new_w_1 = w_1 - lr * ((x) * (sigma(z_1) * (1 - sigma(z_1))) * (w_2) * (sigma(z_2) * (1 - sigma(z_2))) * (-2 *(y - a_2)))\n",
    "print(\"New w_1:\", new_w_1)\n",
    "\n",
    "new_b_1 = b_1 - lr * ((1) * (sigma(z_1) * (1 - sigma(z_1))) * (w_2) * (sigma(z_2) * (1 - sigma(z_2))) * (-2 * (y - a_2)))\n",
    "print(\"New b_1:\", new_b_1)\n",
    "\n",
    "print(\"Update network...\")\n",
    "\n",
    "w_2 = new_w_2\n",
    "b_2 = new_b_2\n",
    "w_1 = new_w_1\n",
    "b_1 = new_b_1\n",
    "\n",
    "z_1 = x * w_1 + b_1\n",
    "a_1 = sigma(z_1)\n",
    "z_2 = a_1 * w_2 + b_2\n",
    "a_2 = sigma(z_2)\n",
    "\n",
    "print(\"y_hat: \", a_2, \"MSE: \", (y-a_2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [array([[1.]], dtype=float32), array([1.], dtype=float32), array([[2.]], dtype=float32), array([2.], dtype=float32)]\n",
      "y_hat: [[0.9772815]]\n",
      "weights: [array([[1.0009537]], dtype=float32), array([1.0009537], dtype=float32), array([[2.004]], dtype=float32), array([2.0045414], dtype=float32)]\n",
      "y_hat: [[0.97746867]]\n"
     ]
    }
   ],
   "source": [
    "# Validation with tensorflow\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.optimizers import SGD\n",
    "\n",
    "x = [1]\n",
    "y = 2\n",
    "w_1 = 1\n",
    "b_1 = 1\n",
    "w_2 = 2\n",
    "b_2 = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=1, use_bias=True, activation=\"sigmoid\"))\n",
    "model.add(Dense(1, use_bias=True, activation=\"sigmoid\"))\n",
    "model.compile(loss='mse', optimizer=SGD(lr=0.1), metrics=['mse'])\n",
    "\n",
    "model.set_weights([[[w_1]], [b_1], [[w_2]], [b_2]])\n",
    "print(\"weights:\", model.get_weights())\n",
    "print(\"y_hat:\", model.predict(np.array(x).reshape(1, 1)))\n",
    "\n",
    "model.fit(np.array([x]), [y], epochs=1, verbose=0)\n",
    "print(\"weights:\", model.get_weights())\n",
    "print(\"y_hat:\", model.predict(np.array(x).reshape(1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: add for multiple neurons in a layer + matrix representation of weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
